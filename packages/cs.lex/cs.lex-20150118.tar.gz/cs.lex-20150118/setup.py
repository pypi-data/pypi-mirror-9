#!/usr/bin/python
from distutils.core import setup
setup(
  name = 'cs.lex',
  description = 'lexical analysis, tokenisers',
  author = 'Cameron Simpson',
  author_email = 'cs@zip.com.au',
  version = '20150118',
  url = 'https://bitbucket.org/cameron_simpson/css/commits/all',
  classifiers = ['Programming Language :: Python', 'Programming Language :: Python :: 2', 'Programming Language :: Python :: 3', 'Topic :: Software Development :: Libraries :: Python Modules', 'Operating System :: OS Independent', 'Development Status :: 4 - Beta', 'License :: OSI Approved :: GNU General Public License v3 (GPLv3)', 'Intended Audience :: Developers'],
  keywords = ['python2', 'python3'],
  long_description = u'Lexical analysis functions, tokenisers.\n=======================================\n\nAn assortment of lexcial and tokenisation functions useful for writing recursive descent parsers, of which I have several.\n\nGenerally the get_* functions accept a source string and an offset (often optional, default 0) and return a token and the new offset, raising ValueError on failed tokenisation.\n\n* as_lines(chunks, partials=None): parse text chunks, yield complete individual lines\n\n* get_chars(s, offset, gochars): collect adjacent characters from `gochars`\n\n* get_delimited(s, offset, delim): collect text up to the first ocurrence of the character `delim`.\n\n* get_envvar(s, offset=0, environ=None, default=None, specials=None): parse an environment variable reference such as $foo\n\n* get_identifier(s, offset=0, alpha=ascii_letters, number=digits, extras=\'_\'): parse an identifier\n\n* get_nonwhite(s, offset=0): collect nonwhitespace characters\n\n* get_other_chars(s, offset=0, stopchars=None): collect adjacent characters not from `stopchars`\n\n* get_qstr(s, offset=0, q=\'"\', environ=None, default=None, env_specials=None): collect a quoted string, honouring slosh escapes and optionally expanding environment variable references\n\n* get_sloshed_text(s, delim, offset=0, slosh=\'\\\\\', mapper=slosh_mapper, specials=None): collect some slosh escaped text with optional special tokens (such as \'$\' introducing \'$foo\')\n\n* get_tokens(s, offset, getters): collect a sequence of tokens specified in `getters`\n\n* match_tokens(s, offset, getters): wrapper for get_tokens which catches ValueError and returns None instead\n\n* get_uc_identifier(s, offset=0, number=digits, extras=\'_\'): collect an UPPERCASE identifier\n\n* get_white(s, offset=0): collect whitespace characters\n\n* isUC_(s): test if a string looks like an upper case identifier\n\n* htmlify(s,nbsp=False): transcribe text in HTML safe form, using &lt; for "<", etc\n\n* htmlquote(s): transcribe text as HTML quoted string suitable for HTML tag attribute values\n\n* jsquote(s): transcribe text as JSON quoted string; essentially like htmlquote without its htmlify step\n\n* parseUC_sAttr(attr): parse FOO or FOOs (or FOOes) and return (FOO, is_plural)\n\n* slosh_mapper(c, charmap=SLOSH_CHARMAP): return a string to replace \\c; the default charmap matches Python slosh escapes\n\n* texthexify(bs, shiftin=\'[\', shiftout=\']\', whitelist=None): a function like binascii.hexlify but also supporting embedded "printable text" subsequences for compactness and human readbility in the result; initial use case was for transcription of binary data with frequent text, specificly directory entry data\n\n* untexthexify(s, shiftin=\'[\', shiftout=\']\'): the inverse of texthexify()\n\n* unctrl(s,tabsize=8): transcribe text removing control characters\n\n* unrfc2047(s): accept RFC2047 encoded text as found in mail message headers and decode\n\n',
  package_dir = {'': 'lib/python'},
  py_modules = ['cs.lex'],
  requires = ['cs.py3'],
)
