# -*- coding: utf-8 -*-

citation = {'ref': u"Anders Krogh, Bj√∂rn Larsson, Gunnar von Heijne and Erik "
                   u"L. L. Sonnhammer (2001) Predicting Transmembrane Protein "
                   u"Topology with a Hidden Markov Model: Application to "
                   u"Complete Genomes. J. Mol. Biol. 305:567-580. \n"
                   u"<http://dx.doi.org/10.1006/jmbi.2000.4315>",
            'name': "TMHMM 2.0"
           }
          
__DEBUG__ = False

import sys, os, time, json, re
from suds.client import Client
from suds.bindings import binding 
import logging
from inmembrane.helpers import log_stderr, print_proteins

def annotate(params, proteins, \
             #url = 'http://www.cbs.dtu.dk/ws/TMHMM/TMHMM_2_0_ws0.wsdl', \
             #url = 'http://www.cbs.dtu.dk/ws/TMHMM/TMHMM_2_0_ws1.wsdl', \
             url = "http://raw.github.com/boscoh/inmembrane/master/inmembrane/plugins/extra/TMHMM_2_0_ws0.wsdl", \
             batchsize = 2000, \
             force=False):
  mapping = {'TMhelix':'tmhmm_helices',\
              'outside':'tmhmm_outer_loops',\
               'inside':'tmhmm_inner_loops',\
  }
  
  if __DEBUG__:
    logging.basicConfig(level=logging.INFO)
    # soap messages (in&out) and http headers
    logging.getLogger('suds.client').setLevel(logging.DEBUG)             
      
  # grab the cached results if present
  outfile = "tmhmm_web.out"
  if not force and os.path.isfile(outfile):
    log_stderr("# -> skipped: %s already exists" % outfile)
    fh = open(outfile, 'r')
    annots = json.loads(fh.read())
    fh.close()
    for seqid in annots:
      for k in mapping.values():
        proteins[seqid][k] = annots[seqid][k]

    citation['name'] = annots[seqid]['program_name']
    return proteins
  
  log_stderr("# TMHMM(web), %s > %s" % (params['fasta'], outfile))
  log_stderr("# TMHMM(web): submitting in batches of %i sequences" % batchsize)
  
  seqids = proteins.keys()
  tmhmm_dict = {}
  while seqids:
    seqid_batch = seqids[0:batchsize]
    del seqids[0:batchsize]
    client = Client(url, cache=None)
    request=client.factory.create('runService.parameters')
    
    # this is a horrible horrible workaround to account for the fact that
    # the lipop SOAP service returns null results if there is are certain
    # non-alphanumeric characters in the sequence id provided. horrible.
    tmhmm_seq_id_mapping = {}
    seqcount = 0
    
    sys.stderr.write("# ")
    for seqid in seqid_batch:
      seq = client.factory.create('runService.parameters.sequencedata.sequence')

      # workaround: removes any non-alphanumeric character (except '_') and adds
      # a unique number to the start to ensure every id is unique after mangling
      newseqid = `seqcount`+re.sub(r'[^\w]', "", seqid)
      seqcount += 1
      tmhmm_seq_id_mapping[newseqid] = seqid
      #seq.id = seqid
      seq.id = newseqid

      seq.seq = proteins[seqid]['seq']
      request.sequencedata.sequence.append(seq)
      sys.stderr.write(".")
      
    response = client.service.runService(request)

    sys.stderr.write("\n")
    
    #pollQueue
    job = client.factory.create('pollQueue.job')
    job.jobid = response.jobid
    response = client.service.pollQueue(job)
    retries = 0
    sys.stderr.write("# Waiting for TMHMM(web) results ")
    while response.status != "FINISHED" and retries < 100:
      response = client.service.pollQueue(job)
      time.sleep(10 + (retries*2))
      retries += 1
      sys.stderr.write(".")
      
      # if something goes wrong, note it and skip TMHMM
      # by returning
      if response.status == "REJECTED" or \
         response.status == "UNKNOWN JOBID" or \
         response.status == "QUEUE DOWN" or \
         response.status == "FAILED":
        log_stderr("TMHMM(web) failed: '%s'" % (response.status))
        return proteins
        
    sys.stderr.write(" done !\n")
      
    #fetchResults
    done_job = client.factory.create('fetchResult.job')
    done_job.jobid = response.jobid
    result = client.service.fetchResult(done_job)
    
    if __DEBUG__: log_stderr(str(result))

    citation["name"] = result[0].method + " " + result[0].version
      
    for res in result.ann:
      #seqid = res.sequence.id
      seqid = tmhmm_seq_id_mapping[res.sequence.id]
      if 'tmhmm_helices' not in proteins[seqid]:
        proteins[seqid].update({
          'tmhmm_helices':[],
          'tmhmm_inner_loops':[],
          'tmhmm_outer_loops':[]
        })
      if len(res.annrecords) > 0:
        for segment in res.annrecords.annrecord:
          if segment.comment in mapping:
            tmhmmkey = mapping[segment.comment]
            proteins[seqid][tmhmmkey].append(\
                           (segment.range.begin, segment.range.end))
                           
      # for caching in the outfile
      if seqid not in tmhmm_dict:
        tmhmm_dict[seqid] = {}

      # extract a copy of results from proteins dictionary
      # ready to we written to cache file
      for k in mapping.values():
        tmhmm_dict[seqid][k] = proteins[seqid][k]

      tmhmm_dict[seqid]['program_name'] = citation['name']
   
  if __DEBUG__: print_proteins(proteins)
  # we store the minimal stuff in JSON format
  fh = open(outfile, 'w')
  fh.write(json.dumps(tmhmm_dict, separators=(',',':\n')))
  fh.close()    
  
  return proteins

